{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyM2nB7SYfaRjPMYwj8yR4Mk"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5AehReSD4Tp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578322468,
     "user_tz": 360,
     "elapsed": 20237,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "5adf00b7-7aaa-4728-cf8f-3c42ff331733",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (5.3.4)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (7.9.0)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.6/132.6 KB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (4.9.2)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (3.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (3.7)\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.2/125.2 KB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (1.3.5)\n",
      "Collecting plotly_express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Collecting pyDAWG\n",
      "  Downloading pyDAWG-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m55.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (1.10.1)\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 20)) (4.65.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (1.8.2.2)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (6.3.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.2)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.1.12)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.7.5)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m54.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (57.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.39.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (2022.6.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (8.1.3)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 11)) (2022.7.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (5.5.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.13.5)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.18-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (2.8.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.1.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 17)) (0.8.10)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m51.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (23.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (15.0.6.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.51.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.4.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 19)) (0.38.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.1.0->plotly_express->-r requirements.txt (line 12)) (8.2.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.16.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis->-r requirements.txt (line 14)) (2.1.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (5.2.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.3.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (6.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2022.12.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.2.2)\n",
      "Building wheels for collected packages: jellyfish, pyDAWG, stop_words, docopt\n",
      "  Building wheel for jellyfish (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp39-cp39-linux_x86_64.whl size=81483 sha256=5404ab84f3993454494602b050e661a2f276e8374084306ff79cac13375b91de\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/28/ba/284e37010e5d3aeed5e45345b58ab8683f97bdce46c9e147f9\n",
      "  Building wheel for pyDAWG (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp39-cp39-linux_x86_64.whl size=65924 sha256=2e9513b5e10e40355ee64e2b70bcd54f4aabb801491fdd6dcabc7b3855a9f472\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/46/09/23af15e75c282523ef4f199c41db4874918b59c21939cd16af\n",
      "  Building wheel for stop_words (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=1f2414a41c5534f157e4c7d396c1807580b259fd0780663172d89181e0cae8a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/d8/66/395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
      "  Building wheel for docopt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=af0b90eff30fdf4f26529eaf0de753ba0d2cbd8f28192d448b94d76fc24a561d\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built jellyfish pyDAWG stop_words docopt\n",
      "Installing collected packages: stop_words, python-crfsuite, pyDAWG, funcy, docopt, sklearn_crfsuite, num2words, jellyfish, jedi, pyLDAvis, plotly_express\n",
      "Successfully installed docopt-0.6.2 funcy-1.18 jedi-0.18.2 jellyfish-0.9.0 num2words-0.5.12 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6 stop_words-2018.7.23\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preprocesado\n",
    "\n",
    "En esta etapa, se realiza el preprocesado de las reseñas para adecuar el formato de las mismas a uno más adecuado para su procesamiento posterior.\n",
    "\n",
    "### Limpieza de texto\n",
    "\n",
    "En primer lugar, se realiza una limpieza básica de las reseñas para eliminar caracteres no alfabéticos y convertir el texto a minúsculas."
   ],
   "metadata": {
    "id": "VqhV8qWU_ns-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cargamos el dataset preprocesado\n",
    "reduced_filtered_data = pd.read_csv('reduced_filtered_data.csv')\n",
    "reduced_filtered_data['reviewText'] = reduced_filtered_data['reviewText'].astype(str)\n"
   ],
   "metadata": {
    "id": "4wFZUJm8AI-T",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578567532,
     "user_tz": 360,
     "elapsed": 889,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fIhk6d4B_EW7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578570056,
     "user_tz": 360,
     "elapsed": 11,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Copiamos en un nuevo dataframe\n",
    "data_lower = reduced_filtered_data.copy()\n",
    "\n",
    "# Aplicamos la función de limpieza a los reviews\n",
    "data_lower['reviewText'] = data_lower['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Guardamos este dataset en un fichero csv\n",
    "data_lower.to_csv('data_lower.csv', index=False)\n"
   ],
   "metadata": {
    "id": "s0RbB9YiAPL9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578576984,
     "user_tz": 360,
     "elapsed": 929,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Cargamos el dataset preprocesado\n",
    "data_lower = pd.read_csv('data_lower.csv')\n",
    "data_lower['reviewText'] = data_lower['reviewText'].astype(str)\n",
    "data_lower.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odBKRfMPAP0c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578622226,
     "user_tz": 360,
     "elapsed": 11,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "a3a45df0-d46d-4f6a-83f2-01308b4696d0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26531, 13)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenización\n",
    "A continuación, se tokenizan las reseñas utilizando la función word_tokenize de la librería nltk."
   ],
   "metadata": {
    "id": "ydcrXNFqAUA5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenizamos usando word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Aplicamos la función a la columna 'review' del dataset\n",
    "data_lower['reviewText'] = data_lower['reviewText'].apply(tokenize_text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W713IHLzARz3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578634048,
     "user_tz": 360,
     "elapsed": 10122,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "649a2658-c491-4ccd-c60a-fa8a00102147",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Eliminación de stopwords\n",
    "\n",
    "Finalmente, se eliminan las palabras vacías (stopwords) de las reseñas utilizando la lista de stopwords de la librería nltk."
   ],
   "metadata": {
    "id": "tG9Prov1Aa0D",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # removemos stopword de la lista de tokens\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# Aplicamos remove_stopwords a la columna review del dataset\n",
    "data_lower['reviewText'] = data_lower['reviewText'].apply(remove_stopwords)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCPxa5a0AX-1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578649318,
     "user_tz": 360,
     "elapsed": 6157,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "7286ffe1-165c-4540-9d74-ca8274360387",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Guardamos este dataset en un fichero csv\n",
    "data_lower.to_csv('data_lower_preprocesado.csv', index=False)"
   ],
   "metadata": {
    "id": "_tlTlHqcAgln",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578657162,
     "user_tz": 360,
     "elapsed": 1921,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": []
  }
 ]
}