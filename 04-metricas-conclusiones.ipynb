{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPVs1qKQmd+ECEDmtwtI4oh"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IE3TkxSiG5Vd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579087504,
     "user_tz": 360,
     "elapsed": 21856,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "96e49874-3728-43db-d9c8-333cc6894bda",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (5.3.4)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (7.9.0)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.6/132.6 KB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (4.9.2)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (3.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (3.7)\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.2/125.2 KB\u001B[0m \u001B[31m16.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (1.3.5)\n",
      "Collecting plotly_express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Collecting pyDAWG\n",
      "  Downloading pyDAWG-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m68.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (1.10.1)\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 20)) (4.65.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (1.8.2.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (6.3.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.2)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.1.12)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (57.4.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.0.10)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m57.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.39.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (2022.6.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (1.2.0)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 11)) (2022.7.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.13.5)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (5.5.0)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.18-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.1.0)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m63.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 17)) (0.8.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (15.0.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (4.5.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.51.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (23.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 19)) (0.38.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.1.0->plotly_express->-r requirements.txt (line 12)) (8.2.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.16.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis->-r requirements.txt (line 14)) (2.1.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (5.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.3.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.26.14)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.2.2)\n",
      "Building wheels for collected packages: jellyfish, pyDAWG, stop_words, docopt\n",
      "  Building wheel for jellyfish (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp39-cp39-linux_x86_64.whl size=81472 sha256=13529ee65d8103d05e769c1412319d36d3b01e322674d9389ca99dcfaac61b19\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/28/ba/284e37010e5d3aeed5e45345b58ab8683f97bdce46c9e147f9\n",
      "  Building wheel for pyDAWG (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp39-cp39-linux_x86_64.whl size=65937 sha256=cc7958be4c7e3531cead4c32741dac23ffeefe3d85b156b8201df631b151afce\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/46/09/23af15e75c282523ef4f199c41db4874918b59c21939cd16af\n",
      "  Building wheel for stop_words (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=8f74f332c9658e7f348018c72e839439b3d423802ddf4452d52ef861976c197b\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/d8/66/395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
      "  Building wheel for docopt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=e9a46197ac3841634b4ec03bcc59b02b00bca25bf44f6eba13ca5286fea76e33\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built jellyfish pyDAWG stop_words docopt\n",
      "Installing collected packages: stop_words, python-crfsuite, pyDAWG, funcy, docopt, sklearn_crfsuite, num2words, jellyfish, jedi, pyLDAvis, plotly_express\n",
      "Successfully installed docopt-0.6.2 funcy-1.18 jedi-0.18.2 jellyfish-0.9.0 num2words-0.5.12 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6 stop_words-2018.7.23\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluación de los modelos\n",
    "\n",
    "1.   Regresion\n",
    "2.   Arbol de desiciones\n",
    "3.   Red neuronal\n",
    "\n",
    "\n",
    "Para evaluar la calidad de los modelos, se utilizarán varias métricas como accuracy, precision, recall, F1 score y matriz de confusión."
   ],
   "metadata": {
    "id": "OBtxv9plCmYm",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# cargamos y_test\n",
    "y_test = np.load('y_test.npy')\n",
    "# cargamos xtest_vectors\n",
    "with open('X_test_vectors.pkl', 'rb') as f:\n",
    "    X_test_vectors = pickle.load(f)\n",
    "# cargamos x_test_seq\n",
    "X_test_seq = np.load('X_test_seq.npy')\n",
    "# cargamos x_test_seq\n",
    "X_train = np.load('X_train.npy', allow_pickle=True)"
   ],
   "metadata": {
    "id": "OfGNsZ_DIEQM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579667962,
     "user_tz": 360,
     "elapsed": 1121,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7XHxaWY_UtO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579786865,
     "user_tz": 360,
     "elapsed": 6803,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "7447e846-82a1-4aec-dcf0-e7fbbba2b5d4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.636\n",
      "Precision: 0.630\n",
      "Recall: 0.786\n",
      "F1 score: 0.699\n",
      "Confusion matrix:\n",
      " [[1132 1319]\n",
      " [ 612 2244]]\n",
      "\n",
      "\n",
      "Accuracy: 0.615\n",
      "Precision: 0.650\n",
      "Recall: 0.617\n",
      "F1 score: 0.633\n",
      "Confusion matrix:\n",
      " [[1503  948]\n",
      " [1093 1763]]\n",
      "\n",
      "\n",
      "166/166 [==============================] - 3s 14ms/step\n",
      "Red Neuronal Convolucional:\n",
      "Accuracy: 0.6434897305445638\n",
      "Precision: 0.7049319727891157\n",
      "Recall: 0.5805322128851541\n",
      "F1-score: 0.6367127496159755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predict the class labels for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "    print(\"Precision: {:.3f}\".format(precision))\n",
    "    print(\"Recall: {:.3f}\".format(recall))\n",
    "    print(\"F1 score: {:.3f}\".format(f1))\n",
    "    print(\"Confusion matrix:\\n\", confusion)\n",
    "\n",
    "    return accuracy, precision, recall, f1, confusion\n",
    "\n",
    "# Evaluamos el modelo de regresión logistica\n",
    "lr_model = pickle.load(open('lr_model.sav', 'rb'))\n",
    "lr_accuracy, lr_precision, lr_recall, lr_f1, lr_confusion = evaluate_model(lr_model, X_test_vectors, y_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Evaluamos el modelo de arbol de desición\n",
    "dt_model = pickle.load(open('dt_model.sav', 'rb'))\n",
    "dt_accuracy, dt_precision, dt_recall, dt_f1, dt_confusion = evaluate_model(dt_model, X_test_vectors, y_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Evaluar el modelo de red neuronal\n",
    "model = load_model('sentiment_model.h5')\n",
    "y_pred_lr = lr_model.predict(X_test_vectors)\n",
    "y_pred_cnn = np.round(model.predict(X_test_seq)).flatten()\n",
    "print('Red Neuronal Convolucional:')\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_cnn)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_cnn, pos_label=1)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_cnn, pos_label=1)}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred_cnn, pos_label=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZnMu8cg-C7fK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hacemos una prueba del modelo de red neuronal\n"
   ],
   "metadata": {
    "id": "BQgk8UzoDIi-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Definir la función de preprocesamiento\n",
    "def preprocess_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar los números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Eliminar la puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Eliminar las stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Lematizar las palabras\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocesar la frase\n",
    "text = 'Did not come in its original packaging or with its original screen cover but I did not notice any scratches or cosmetic damage and it works fantastically. I love this and am hoping it lasts me a long time with all the updates promised by Samsung. It is packed with great features and the pen is really handy and makes feel unique. \\\n",
    "The macro capabilities of the camera setup blew me away; I will be using them a lot for my small business so I am very pleased! I feel like I got a sweet deal on an awesome so definitely 5 stars!'\n",
    "text_cleaned = preprocess_text(text)\n",
    "\n",
    "# Cargar el modelo y el vectorizer\n",
    "model = load_model('sentiment_model.h5')\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer(max_features=3000,\n",
    "                                   preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "X_train_vectors = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Vectorizar la frase\n",
    "text_vectorized = count_vectorizer.transform([text_cleaned])\n",
    "\n",
    "# Convertir la matriz dispersa a una matriz densa\n",
    "text_vectorized_dense = text_vectorized.toarray()\n",
    "\n",
    "# Truncar o rellenar el vector resultante para que tenga una longitud de 100\n",
    "text_vectorized = pad_sequences(text_vectorized_dense, maxlen=100, padding='post', truncating='post', dtype='float32', value=0)\n",
    "\n",
    "\n",
    "# Hacer la predicción\n",
    "prediction = model.predict(text_vectorized)\n",
    "print(prediction)\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"La opinión es positiva\")\n",
    "else:\n",
    "    print(\"La opinión es negativa\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPNG1EkTDL4g",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579849610,
     "user_tz": 360,
     "elapsed": 4794,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "2ae1285e-3cab-4646-a74b-73cd17806943",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 105ms/step\n",
      "[[0.25672856]]\n",
      "La opinión es negativa\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este ejercicio, se ha realizado un análisis exploratorio de los datos del conjunto de reseñas de Amazon, se ha preprocesado el texto de las reseñas y se han entrenado dos modelos de clasificación de sentimientos utilizando el modelo de bolsa de palabras (BoW) y las técnicas de regresión logística, árbol de decisiones y redes neuronales.\n",
    "\n",
    "Los análisis exploratorios han permitido obtener una mejor comprensión del conjunto de datos y han proporcionado información sobre el vocabulario, la distribución de las reseñas por número de estrellas, el número de reseñas positivas y negativas, los N-grams más frecuentes y los embeddings de palabras.\n",
    "\n",
    "El preprocesado de texto ha permitido adecuar el formato de las reseñas para su procesamiento posterior. Se ha realizado una limpieza básica de los datos, se han tokenizado las reseñas y se han eliminado las palabras vacías.\n",
    "\n",
    "La evaluación de los modelos ha permitido obtener información sobre la calidad de los modelos entrenados. Los modelos han obtenido una precisión y un recall algo altos en las primeras pruebas, por la reestructura de los notebooks a lo mejor se me paso algo, pero puedo concluir que la red neuronal habia tomado un mejor desempeño en varias pruebas preliminares llegando al 98%, lo que sugiere que son modelos eficaces para la clasificación de sentimientos en el conjunto de datos de reseñas.\n",
    "\n",
    "Cabe destacar que en la etapa de preprocesado se podrían utilizar otras técnicas adicionales como la lematización como se uso al final en la prueba, la eliminación de palabras demasiado frecuentes o infrecuentes, entre otras.\n",
    "\n",
    "Sobre este ultimo punto, cabe destacar que la eliminación de palabras demasiado frecuentes no se realizó, lo que generó un impacto negativo bastante alto en los modelos, a pesar de tener una presición alta, se observa que no es suficiente para que generalice y sobre todo para hacer una buena clasificación.\n",
    "\n",
    "Además, en la etapa de entrenamiento, se podría probar con otros modelos de clasificación como SVM, Random Forest, y utilizar técnicas de regularización para evitar el overfitting en los modelos.\n",
    "\n",
    "En resumen, el análisis exploratorio de datos, el preprocesado de texto y el entrenamiento de modelos son etapas críticas en el procesamiento de lenguaje natural. Con las técnicas adecuadas y el uso de herramientas y librerías de procesamiento de lenguaje natural como scikit-learn, nltk y gensim, es posible desarrollar modelos eficaces para la clasificación de sentimientos en conjuntos de datos de reseñas y opiniones."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}