{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNYqtXRLCyXqmyhAVVFCR5q"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import pickle\n",
    "! pip install -r requirements.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHP0abLZFowA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578750880,
     "user_tz": 360,
     "elapsed": 31768,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "8e156bd8-e9aa-41da-93e5-fd9b77d8e649",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (5.3.4)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (7.9.0)\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m132.6/132.6 KB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (4.9.2)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (3.5.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (3.7)\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.2/125.2 KB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (1.3.5)\n",
      "Collecting plotly_express\n",
      "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n",
      "Collecting pyDAWG\n",
      "  Downloading pyDAWG-1.0.1.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m29.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (1.10.1)\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 20)) (4.65.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (1.8.2.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from gensim->-r requirements.txt (line 1)) (6.3.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.2)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (5.7.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->-r requirements.txt (line 2)) (6.1.12)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (57.4.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (0.7.5)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m52.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython->-r requirements.txt (line 3)) (2.0.10)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.39.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 7)) (8.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk->-r requirements.txt (line 8)) (2022.6.2)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 11)) (2022.7.1)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (5.5.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.5.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from plotly_express->-r requirements.txt (line 12)) (0.13.5)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (2.8.4)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.18-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->-r requirements.txt (line 15)) (3.1.0)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m63.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite->-r requirements.txt (line 17)) (0.8.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (15.0.6.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.31.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (23.3.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.51.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.19.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->-r requirements.txt (line 19)) (2.11.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 19)) (0.38.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.1.0->plotly_express->-r requirements.txt (line 12)) (8.2.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->-r requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.4.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.16.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.2.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis->-r requirements.txt (line 14)) (2.1.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (5.2.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.3.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (6.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->-r requirements.txt (line 19)) (3.2.2)\n",
      "Building wheels for collected packages: jellyfish, pyDAWG, stop_words, docopt\n",
      "  Building wheel for jellyfish (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp39-cp39-linux_x86_64.whl size=81469 sha256=cca4786a2680257d903912bb6a097f3d0dd779086af940726a280367a3bbe2f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/a6/28/ba/284e37010e5d3aeed5e45345b58ab8683f97bdce46c9e147f9\n",
      "  Building wheel for pyDAWG (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp39-cp39-linux_x86_64.whl size=65912 sha256=fef1571d40c68056cd69b6ebe9e43c999de04ceb5c7280b8a03e25940e184bbd\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/46/09/23af15e75c282523ef4f199c41db4874918b59c21939cd16af\n",
      "  Building wheel for stop_words (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32910 sha256=4dbf3017ff5d7ecc0ef22ef24547b2643c90903dacae147c7be67b468f0bb052\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/d8/66/395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
      "  Building wheel for docopt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=adbf9116ad73793ba590bb52e71ed6bb0ebbed27fd0a7116df4d8a015c394c60\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built jellyfish pyDAWG stop_words docopt\n",
      "Installing collected packages: stop_words, python-crfsuite, pyDAWG, funcy, docopt, sklearn_crfsuite, num2words, jellyfish, jedi, pyLDAvis, plotly_express\n",
      "Successfully installed docopt-0.6.2 funcy-1.18 jedi-0.18.2 jellyfish-0.9.0 num2words-0.5.12 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6 stop_words-2018.7.23\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Entrenamiento/test\n",
    "En esta etapa, se entrenan dos modelos de clasificación de sentimientos utilizando el conjunto de reseñas preprocesado.\n",
    "\n",
    "### Preparación de los datos\n",
    "\n",
    "En primer lugar, se divide el conjunto de datos en un conjunto de entrenamiento y un conjunto de test."
   ],
   "metadata": {
    "id": "kvAkp779BAWW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F02yg7vY_Nog",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578754529,
     "user_tz": 360,
     "elapsed": 590,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "f664bb33-d25d-44a0-a173-a6a8b18d31aa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26531, 13)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data_lower = pd.read_csv('data_lower_preprocesado.csv')\n",
    "data_lower['reviewText'] = data_lower['reviewText'].astype(str)\n",
    "data_lower.shape"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos en train y test a 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_lower['reviewText'], data_lower['sentiment'], test_size=0.2, random_state=0)\n",
    "# Guardamos y_test para otro notebook\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('X_train.npy', X_train)"
   ],
   "metadata": {
    "id": "zuVG5v1TBQDY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579557100,
     "user_tz": 360,
     "elapsed": 7,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorización del texto\n",
    "A continuación, se vectoriza el texto utilizando el modelo de bolsa de palabras (BoW) con la clase CountVectorizer de scikit-learn."
   ],
   "metadata": {
    "id": "XUaR-uRgBWSq",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creamos un objeto CountVectorizer \n",
    "count_vectorizer = CountVectorizer(max_features=3000,\n",
    "                                   preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
    "\n",
    "# Ajustamos los datos de entrenamiento y obtenemos la matriz\n",
    "X_train_vectors = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transformamos test y obtenemos la matriz. \n",
    "#### Note we are not fitting the test data into the CountVectorizer()\n",
    "X_test_vectors = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Guardar la variable en un archivo\n",
    "with open('X_test_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_vectors, f)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDpaEtl5BQKw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579320804,
     "user_tz": 360,
     "elapsed": 585,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "aeaef330-eef4-4e20-b187-0dd8c72414fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Entrenamiento de los modelos\n",
    "\n",
    "A continuación, se entrenarán dos modelos de clasificación de sentimientos: un modelo de regresión logística y un modelo de árbol de decisiones."
   ],
   "metadata": {
    "id": "OQeCW0viBb2u",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Modelo de regresión logística\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "# Instanciamos con parametros por defecto\n",
    "lr_model = LogisticRegression(random_state=0, max_iter=100)\n",
    "\n",
    "# entrenamos\n",
    "lr_model.fit(X_train_vectors, y_train)\n",
    "#Guardamos \n",
    "filename = 'lr_model.sav'\n",
    "pickle.dump(lr_model, open(filename, 'wb'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zda9bEr5BcrH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578912486,
     "user_tz": 360,
     "elapsed": 1020,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "a554cf3f-54e3-4a0b-9565-e10a9aa73c5e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Arbol de desiciones\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instanciamos el arbol de desicipnes con max_depth=5 y random_state=0\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "# Entrenamos\n",
    "dt_model.fit(X_train_vectors, y_train)\n",
    "# Guardamos\n",
    "filename = 'dt_model.sav'\n",
    "pickle.dump(dt_model, open(filename, 'wb'))"
   ],
   "metadata": {
    "id": "tzBZZ7zABg4a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678578918362,
     "user_tz": 360,
     "elapsed": 1080,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Red neuronal \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Entrenar un modelo de Red Neuronal Convolucional (CNN)\n",
    "max_features = 20000  # Número máximo de palabras a considerar en el vocabulario\n",
    "maxlen = 100  # Longitud máxima de una opinión (en palabras)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "np.save('X_test_seq.npy', X_test_seq)\n",
    "embedding_dim = 100  # Dimensión del espacio de embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_data=(X_test_seq, y_test))\n",
    "# Guardamos nuestro modelo de red neuronal, para cargarlo en un ejemplo\n",
    "model.save('sentiment_model.h5')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukU3P-DKCXg9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579200820,
     "user_tz": 360,
     "elapsed": 268108,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "outputId": "743ff8ee-4e15-4ce4-c3da-531d1aa3ab6b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "664/664 [==============================] - 48s 70ms/step - loss: 0.3490 - accuracy: 0.8450 - val_loss: 0.2692 - val_accuracy: 0.8898\n",
      "Epoch 2/5\n",
      "664/664 [==============================] - 44s 66ms/step - loss: 0.1972 - accuracy: 0.9291 - val_loss: 0.2800 - val_accuracy: 0.8886\n",
      "Epoch 3/5\n",
      "664/664 [==============================] - 48s 72ms/step - loss: 0.1175 - accuracy: 0.9612 - val_loss: 0.3164 - val_accuracy: 0.8851\n",
      "Epoch 4/5\n",
      "664/664 [==============================] - 43s 65ms/step - loss: 0.0675 - accuracy: 0.9790 - val_loss: 0.4114 - val_accuracy: 0.8834\n",
      "Epoch 5/5\n",
      "664/664 [==============================] - 48s 72ms/step - loss: 0.0441 - accuracy: 0.9853 - val_loss: 0.4995 - val_accuracy: 0.8860\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "np.save('X_test_seq.npy', X_test_seq)"
   ],
   "metadata": {
    "id": "Ak6CaMpiIbWs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1678579458822,
     "user_tz": 360,
     "elapsed": 10,
     "user": {
      "displayName": "Rogelio Bautista",
      "userId": "09685885284832322578"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "t3XrZDJmIdan",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}